% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.3 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\datalist[entry]{nty/global//global/global/global}
  \entry{aloiseNPhardnessEuclideanSumofsquares2009}{article}{}{}
    \name{author}{4}{}{%
      {{hash=AD}{%
         family={Aloise},
         familyi={A\bibinitperiod},
         given={Daniel},
         giveni={D\bibinitperiod},
      }}%
      {{hash=DA}{%
         family={Deshpande},
         familyi={D\bibinitperiod},
         given={Amit},
         giveni={A\bibinitperiod},
      }}%
      {{hash=HP}{%
         family={Hansen},
         familyi={H\bibinitperiod},
         given={Pierre},
         giveni={P\bibinitperiod},
      }}%
      {{hash=PP}{%
         family={Popat},
         familyi={P\bibinitperiod},
         given={Preyas},
         giveni={P\bibinitperiod},
      }}%
    }
    \strng{namehash}{AD+1}
    \strng{fullhash}{ADDAHPPP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{abstract}{%
    A recent proof of NP-hardness of Euclidean sum-of-squares clustering, due to Drineas et al. (Mach. Learn. 56:9–33, 2004), is not valid. An alternate short proof is provided.%
    }
    \verb{doi}
    \verb 10.1007/s10994-009-5103-0
    \endverb
    \field{issn}{0885-6125, 1573-0565}
    \field{number}{2}
    \field{pages}{245\bibrangedash 248}
    \field{shortjournal}{Mach Learn}
    \field{title}{NP-Hardness of Euclidean Sum-of-Squares Clustering}
    \verb{url}
    \verb http://link.springer.com/10.1007/s10994-009-5103-0
    \endverb
    \field{volume}{75}
    \field{langid}{english}
    \verb{file}
    \verb /Users/pschafer/Zotero/storage/D8IEQVE2/Aloise et al. - 2009 - NP-har
    \verb dness of Euclidean sum-of-squares clustering.pdf
    \endverb
    \field{journaltitle}{Machine Learning}
    \field{month}{05}
    \field{year}{2009}
    \field{urlday}{07}
    \field{urlmonth}{12}
    \field{urlyear}{2024}
  \endentry

  \entry{clarksonCoresetsSparseGreedy2010}{article}{}{}
    \name{author}{1}{}{%
      {{hash=CKL}{%
         family={Clarkson},
         familyi={C\bibinitperiod},
         given={Kenneth\bibnamedelima L.},
         giveni={K\bibinitperiod\bibinitdelim L\bibinitperiod},
      }}%
    }
    \keyw{/unread}
    \strng{namehash}{CKL1}
    \strng{fullhash}{CKL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    The problem of maximizing a concave function f(x) in the unit simplex Δ can be solved approximately by a simple greedy algorithm. For given k, the algorithm can find a point x(k) on a k-dimensional face of Δ, such that f(x(k) ≥ f(x*) − O(1/k). Here f(x*) is the maximum value of f in Δ, and the constant factor depends on f. This algorithm and analysis were known before, and related to problems of statistics and machine learning, such as boosting, regression, and density mixture estimation. In other work, coming from computational geometry, the existence of ϵ-coresets was shown for the minimum enclosing ball problem by means of a simple greedy algorithm. Similar greedy algorithms, which are special cases of the Frank-Wolfe algorithm, were described for other enclosure problems. Here these results are tied together, stronger convergence results are reviewed, and several coreset bounds are generalized or strengthened.%
    }
    \verb{doi}
    \verb 10.1145/1824777.1824783
    \endverb
    \field{issn}{1549-6325}
    \field{number}{4}
    \field{pages}{63:1\bibrangedash 63:30}
    \field{shortjournal}{ACM Trans. Algorithms}
    \field{title}{Coresets, Sparse Greedy Approximation, and the Frank-Wolfe Algorithm}
    \verb{url}
    \verb https://doi.org/10.1145/1824777.1824783
    \endverb
    \field{volume}{6}
    \field{langid}{english}
    \verb{file}
    \verb /Users/pschafer/Zotero/storage/PA5WL7NE/Clarkson - 2010 - Coresets, s
    \verb parse greedy approximation, and the Frank-Wolfe algorithm.pdf
    \endverb
    \field{journaltitle}{ACM Trans. Algorithms}
    \field{day}{03}
    \field{month}{09}
    \field{year}{2010}
    \field{urlday}{11}
    \field{urlmonth}{02}
    \field{urlyear}{2025}
  \endentry

  \entry{cutlerArchetypalAnalysis1994}{article}{}{}
    \name{author}{2}{}{%
      {{hash=CA}{%
         family={Cutler},
         familyi={C\bibinitperiod},
         given={Adele},
         giveni={A\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Breiman},
         familyi={B\bibinitperiod},
         given={Leo},
         giveni={L\bibinitperiod},
      }}%
    }
    \strng{namehash}{CABL1}
    \strng{fullhash}{CABL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    Archetypal analysis represents each individual in a data set as a mixture of individuals of pure type or archetypes. The archetypes themselves are restricted to being mixtures of the individuals in the data set. Archetypes are selected by minimizing the squared error in representing each individual as a mixture of archetypes. The usefulness of archetypal analysis is illustrated on several data sets. Computing the archetypes is a nonlinear least squares problem, which is solved using an alternating minimizing algorithm.%
    }
    \verb{doi}
    \verb 10.1080/00401706.1994.10485840
    \endverb
    \field{issn}{0040-1706}
    \field{number}{4}
    \field{pages}{338\bibrangedash 347}
    \field{shortjournal}{Technometrics}
    \field{title}{Archetypal Analysis}
    \field{volume}{36}
    \verb{file}
    \verb /Users/pschafer/Zotero/storage/HI2K6PX3/1994_Cutler_Breiman_Archetypa
    \verb l Analysis_Technometrics.pdf
    \endverb
    \field{journaltitle}{Technometrics : a journal of statistics for the physical, chemical, and engineering sciences}
    \field{year}{1994}
  \endentry

  \entry{eggertSparseCodingNMF2004}{inproceedings}{}{}
    \name{author}{2}{}{%
      {{hash=EJ}{%
         family={Eggert},
         familyi={E\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=KE}{%
         family={Korner},
         familyi={K\bibinitperiod},
         given={E.},
         giveni={E\bibinitperiod},
      }}%
    }
    \keyw{Additives,Cost function,Encoding,Europe,Matrix decomposition,Principal component analysis,Sparse matrices,Time factors,Vector quantization,Visual system}
    \strng{namehash}{EJKE1}
    \strng{fullhash}{EJKE1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{E}
    \field{sortinithash}{E}
    \field{abstract}{%
    Non-negative matrix factorization (NMF) is a very efficient parameter-free method for decomposing multivariate data into strictly positive activations and basis vectors. However, the method is not suited for overcomplete representations, where usually sparse coding paradigms apply. We show how to merge the concepts of non-negative factorization with sparsity conditions. The result is a multiplicative algorithm that is comparable in efficiency to standard NMF, but that can be used to gain sensible solutions in the overcomplete cases. This is of interest e.g. for the case of learning and modeling of arrays of receptive fields arranged in a visual processing map, where an overcomplete representation is unavoidable.%
    }
    \field{booktitle}{2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)}
    \verb{doi}
    \verb 10.1109/IJCNN.2004.1381036
    \endverb
    \field{issn}{1098-7576}
    \field{pages}{2529\bibrangedash 2533 vol.4}
    \field{title}{Sparse Coding and NMF}
    \verb{url}
    \verb https://ieeexplore.ieee.org/document/1381036
    \endverb
    \field{volume}{4}
    \verb{file}
    \verb /Users/pschafer/Zotero/storage/RG3YUI7H/Eggert and Korner - 2004 - Sp
    \verb arse coding and NMF.pdf;/Users/pschafer/Zotero/storage/AIAA6KYG/13810
    \verb 36.html
    \endverb
    \field{month}{07}
    \field{year}{2004}
    \field{urlday}{30}
    \field{urlmonth}{03}
    \field{urlyear}{2025}
  \endentry

  \entry{linProjectedGradientMethods2007}{article}{}{}
    \name{author}{1}{}{%
      {{hash=LCJ}{%
         family={Lin},
         familyi={L\bibinitperiod},
         given={Chih-Jen},
         giveni={C\bibinithyphendelim J\bibinitperiod},
      }}%
    }
    \strng{namehash}{LCJ1}
    \strng{fullhash}{LCJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    Nonnegative matrix factorization (NMF) can be formulated as a minimization problem with bound constraints. Although bound-constrained optimization has been studied extensively in both theory and practice, so far no study has formally applied its techniques to NMF. In this letter, we propose two projected gradient methods for NMF, both of which exhibit strong optimization properties. We discuss efficient implementations and demonstrate that one of the proposed methods converges faster than the popular multiplicative update approach. A simple Matlab code is also provided.%
    }
    \verb{doi}
    \verb 10.1162/neco.2007.19.10.2756
    \endverb
    \field{eventtitle}{Neural Computation}
    \field{issn}{0899-7667}
    \field{number}{10}
    \field{pages}{2756\bibrangedash 2779}
    \field{title}{Projected Gradient Methods for Nonnegative Matrix Factorization}
    \verb{url}
    \verb https://ieeexplore.ieee.org/document/6795860
    \endverb
    \field{volume}{19}
    \verb{file}
    \verb /Users/pschafer/Zotero/storage/AUREHXIV/Lin - 2007 - Projected Gradie
    \verb nt Methods for Nonnegative Matrix Factorization.pdf;/Users/pschafer/Z
    \verb otero/storage/PPBZYNW6/6795860.html
    \endverb
    \field{journaltitle}{Neural Computation}
    \field{month}{10}
    \field{year}{2007}
    \field{urlday}{30}
    \field{urlmonth}{03}
    \field{urlyear}{2025}
  \endentry

  \entry{morupArchetypalAnalysisMachine2012}{article}{}{}
    \name{author}{2}{}{%
      {{hash=MM}{%
         family={Mørup},
         familyi={M\bibinitperiod},
         given={Morten},
         giveni={M\bibinitperiod},
      }}%
      {{hash=HLK}{%
         family={Hansen},
         familyi={H\bibinitperiod},
         given={Lars\bibnamedelima Kai},
         giveni={L\bibinitperiod\bibinitdelim K\bibinitperiod},
      }}%
    }
    \strng{namehash}{MMHLK1}
    \strng{fullhash}{MMHLK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{abstract}{%
    Archetypal analysis (AA) proposed by Cutler and Breiman (1994) [7] estimates the principal convex hull (PCH) of a data set. As such AA favors features that constitute representative ‘corners’ of the data, i.e., distinct aspects or archetypes. We currently show that AA enjoys the interpretability of clustering –without being limited to hard assignment and the uniqueness of SVD – without being limited to orthogonal representations. In order to do large scale AA, we derive an efficient algorithm based on projected gradient as well as an initialization procedure we denote FURTHESTSUM that is inspired by the FURTHESTFIRST approach widely used for K-means (Hochbaum and Shmoys, 1985 [14]). We generalize the AA procedure to KERNEL-AA in order to extract the principal convex hull in potential infinite Hilbert spaces and derive a relaxation of AA when the archetypes cannot be represented as convex combinations of the observed data. We further demonstrate that the AA model is relevant for feature extraction and dimensionality reduction for a large variety of machine learning problems taken from computer vision, neuroimaging, chemistry, text mining and collaborative filtering leading to highly interpretable representations of the dynamics in the data. Matlab code for the derived algorithms is available for download from www.mortenmorup.dk.%
    }
    \verb{doi}
    \verb 10.1016/j.neucom.2011.06.033
    \endverb
    \field{issn}{09252312}
    \field{pages}{54\bibrangedash 63}
    \field{shortjournal}{Neurocomputing}
    \field{title}{Archetypal Analysis for Machine Learning and Data Mining}
    \verb{url}
    \verb https://linkinghub.elsevier.com/retrieve/pii/S0925231211006060
    \endverb
    \field{volume}{80}
    \field{langid}{english}
    \verb{file}
    \verb /Users/pschafer/Zotero/storage/HHR85VMR/Mørup and Hansen - 2012 - Ar
    \verb chetypal analysis for machine learning and data mining.pdf
    \endverb
    \field{journaltitle}{Neurocomputing}
    \field{month}{03}
    \field{year}{2012}
    \field{urlday}{07}
    \field{urlmonth}{12}
    \field{urlyear}{2024}
  \endentry

  \entry{petersenMatrixCookbook2012}{book}{}{}
    \name{author}{2}{}{%
      {{hash=PKB}{%
         family={Petersen},
         familyi={P\bibinitperiod},
         given={Kaare\bibnamedelima Brandt},
         giveni={K\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
      {{hash=PMS}{%
         family={Pedersen},
         familyi={P\bibinitperiod},
         given={Michael\bibnamedelima Syskind},
         giveni={M\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Technical University of Denmark}%
    }
    \keyw{/unread,inverse,matrix derivative,Matrix identity,matrix relations}
    \strng{namehash}{PKBPMS1}
    \strng{fullhash}{PKBPMS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{abstract}{%
    Matrix identities, relations and approximations. A desktop reference for quick overview of mathematics of matrices.%
    }
    \field{title}{The Matrix Cookbook}
    \verb{url}
    \verb https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf
    \endverb
    \field{langid}{english}
    \field{year}{2012}
  \endentry
\enddatalist
\endinput
