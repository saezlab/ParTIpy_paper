\documentclass[oneside]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

% This package offers a versatile interface to add pictures to the document
\usepackage{graphicx}

% This specifies that the page and heading is part of the header of each page
\pagestyle{headings}
\usepackage{blindtext}

% math stuff
\usepackage{amsmath}
\usepackage{amssymb}

% algorithm pseudocode
\usepackage{algorithm}
\usepackage{algpseudocode}

% Specify the T1 font encoding, which is a 8-bit encoding (256 glyphs) ensuring automatic hyphenation amongst other things
% The latex default is OT1 which is a 7-bit encoding (128 glyphs)
\usepackage[T1]{fontenc}

% Geometry package offers an easy interface to adapt the layout of the document, here is use total to specify the dimensions of the body
\usepackage[a4paper, total={16cm, 23cm}]{geometry}

% This package is useful to enter units
\usepackage{siunitx}

\usepackage[
  backend=bibtex,
  style=numeric,       % Numeric citations
  %autocite=superscript, % Superscript in-text numbers
  url=true,            % Ensure URLs are processed
  sortcites=true,
]{biblatex}
\usepackage{url}       % Better URL formatting
\addbibresource{ref.bib}

% Include URLs in the bibliography entries
\DeclareFieldFormat{url}{\url{#1}}
\renewbibmacro{finentry}{
  \iffieldundef{url}
    {\finentry}
    {\setunit{\addperiod\space}%
     \printfield{url}
     \finentry}}

% Avoid warning: "babel/polyglossia" detected but "csquotes" missing, loading csquotes recommended
\usepackage{csquotes}

% It is recommended to use this package to improve figure placement
% https://tex.stackexchange.com/questions/8625/force-figure-placement-in-text
\usepackage{float}

% Recommended package to align text
\usepackage{ragged2e}

% Create new command for equal sign with less space around them
% from: https://tex.stackexchange.com/questions/365261/how-to-neatly-space-the-equals-sign-when-using-probabilities
\newcommand\myeq{\mkern1.5mu{=}\mkern1.5mu}

% new command for inline code highlighting
% from https://tex.stackexchange.com/questions/286094/insert-code-keywords-inline
\NewDocumentCommand{\codeword}{v}{%
\texttt{\textcolor{black}{#1}}%
}

% Creating hyperlinks when referencing
\usepackage{hyperref}

% Package that makes it easier to reference and insert references as hyperlinks
% It should also be the last package to be imported
% To be able to reference an object (section, figure) I have to give it a label via \label{xyz}
% Then I can reference: 1. Name - \nameref{}, 2. Page - \pageref{}, 
% Some packages to create nice table, according to:
% https://www.tablesgenerator.com
\usepackage{booktabs}

% General Remarks regarding figures
% figures created with inkscape (e.g. gel, microscopy): Serif Font, exported as pdf
% figures created with R/ggplot (e.g. barplot before trimming): Serif Font, exported as pdf

% Set the intend and spacing of paragraphs
\setlength{\parindent}{0ex}
\setlength{\parskip}{1em}

\title{Archetypal Analysis}
\author{Philipp Sven Lars Schäfer}
\date{April 2025}

\begin{document}

\maketitle

\section{Problem Formulation}

Let $\mathcal{X}=\{\mathbf{x}_1, \ldots, \mathbf{x}_N\}_{n=1}^N$ be a data set consisting of $N$ $D$-dimensional data points, and let $\mathbf{X} \in \mathbb{R}^{N \times D}$ be the matrix where each row is a data point.

In Archetypal Analysis we make two assumptions: 1) Each data point is a convex combination of $K$ archetypes; 2) Each archetype is a convex combination of $N$ data points.

Expressing the first assumption in matrix notation yields
\begin{equation}
\label{eq:first-assumption}
\hat{\mathbf{X}} = \mathbf{A} \mathbf{Z}
\end{equation}
where $\hat{\mathbf{X}} \in \mathbb{R}^{N \times D}$ is the reconstructed data matrix, $\mathbf{Z} \in \mathbb{R}^{K \times D}$ is the matrix of archetypes (i.e. each row is one archetype), and $\mathbf{A} \in \mathbb{R}^{N \times K}$ is a row-stochastic matrix that defines 

Expressing the second assumption in matrix notation yields
\begin{equation}
\label{eq:second-assumption}
\mathbf{Z} = \mathbf{B} \mathbf{X}
\end{equation}
where $\mathbf{B} \in \mathbb{R}^{K \times N}$ is a row-stochastic matrix that defines 

The reconstruction error is most commonly measured using the residual sum of squares (RSS), given by the squared Frobenius norm,

\begin{equation}
\label{eq:rss}
\| \mathbf{X} - \hat{\mathbf{X}} \|_F^2 = \| \mathbf{X} - \mathbf{A} \mathbf{Z} \|_F^2 =  \| \mathbf{X} -  \mathbf{A} \mathbf{B} \mathbf{X} \|_F^2
\end{equation}
which yields the following optimization objective
\begin{equation}
\label{eq:objective}
\begin{aligned}
\mathbf{A}^\star, \mathbf{B}^\star =& \; \underset{\begin{subarray}{c} \mathbf{A} \in \mathbb{R}^{N \times K} \\ 
    \mathbf{B} \in \mathbb{R}^{K \times N} \end{subarray}}{\arg \min} \| \mathbf{X} - \mathbf{A} \mathbf{B} \mathbf{X}\|_F^2 \quad \text{ subject to} \\
& \mathbf{A} \geq 0, \mathbf{A} \mathbf{1}_K = \mathbf{1}_N \\
& \mathbf{B} \geq 0, \mathbf{B} \mathbf{1}_N = \mathbf{1}_K
\end{aligned}
\end{equation}

Introducing the set of row-stochastic non-negative matrices,

\begin{equation}
    F(N, K) := \left\{ \mathbf{A} \in \mathbb{R}^{N \times K} \mid  \mathbf{A} \geq 0 \land \mathbf{A} \mathbf{1}_K = \mathbf{1}_N \right\}
\end{equation}

we can write the objective more compactly as:

\begin{equation}
    \begin{aligned}
    \mathbf{A}^\star, \mathbf{B}^\star =& \; \underset{\begin{subarray}{c} \mathbf{A} \in F(N, K) \\ 
        \mathbf{B} \in F(K, N) \end{subarray}}{\arg \min} \| \mathbf{X} - \mathbf{A} \mathbf{B} \mathbf{X}\|_F^2
    \end{aligned}
\end{equation}

%We can equivalently write the objective as ... (min with over x where x in conv hull of the current archetypes)

\section{Properties of the Objective}

Property 1 (Translation invariance): The minimizers $\mathbf{A}^\star, \mathbf{B}^\star$ of the objective are invariant under row-wise translations of $\mathbf{X}$. Let $\tilde{\mathbf{X}} = \mathbf{X} + \mathbf{1}_N \mathbf{v}^T$ for any $\mathbf{v} \in \mathbb{R}^D$, then

\begin{equation}
    \underset{\begin{subarray}{c} \mathbf{A} \in F(N, K) \\ \mathbf{B} \in F(K, N) \end{subarray}}{\arg \min} \| \tilde{\mathbf{X}} - \mathbf{A} \mathbf{B} \tilde{\mathbf{X}} \|_F^2 
    = 
    \underset{\begin{subarray}{c} \mathbf{A} \in F(N, K) \\ \mathbf{B} \in F(K, N) \end{subarray}}{\arg \min} \| \mathbf{X} - \mathbf{A} \mathbf{B} \mathbf{X}\|_F^2 
\end{equation}

Proof: Let $\mathbf{v} \in \mathbb{R}^{D}$, and let $\tilde{\mathbf{X}} = \mathbf{X} + \mathbf{1}_N \mathbf{v}^T$ be the translated matrix. Then for any feasible $\mathbf{A}, \mathbf{B}$

\begin{equation}
    \begin{aligned}
    \tilde{\mathbf{X}} - \mathbf{A} \mathbf{B} \tilde{\mathbf{X}} 
    &= \left(\mathbf{X} + \mathbf{1}_N \mathbf{v}^T \right) - \mathbf{A} \mathbf{B} \left(\mathbf{X} + \mathbf{1}_N \mathbf{v}^T \right) \\
    &= \mathbf{X} + \mathbf{1}_N \mathbf{v}^T - \mathbf{A} \mathbf{B} \mathbf{X} -  \mathbf{A} \mathbf{B} \mathbf{1}_N \mathbf{v}^T
    \end{aligned}
\end{equation}

Since $\mathbf{B} \mathbf{1}_N = \mathbf{1}_K$ and  $\mathbf{A} \mathbf{1}_K = \mathbf{1}_N$, this simplifies to 

\begin{equation}
    \begin{aligned}
    \tilde{\mathbf{X}} - \mathbf{A} \mathbf{B} \tilde{\mathbf{X}}
    &= \mathbf{X} + \mathbf{1}_N \mathbf{v}^T - \mathbf{A} \mathbf{B} \mathbf{X} - \mathbf{1}_N \mathbf{v}^T \\
    &= \mathbf{X} - \mathbf{A} \mathbf{B} \mathbf{X}
    \end{aligned}
\end{equation}

Therefore, the reconstruction error remains unchanged, and the minimizers $\mathbf{A}^\star, \mathbf{B}^\star$ are invariant under such translations.

Property 2 (Scale invariance): The minimizers $\mathbf{A}^\star, \mathbf{B}^\star$ of the objective are invariant under global scaling of $\mathbf{X}$. Let $\tilde{\mathbf{X}} = \lambda \mathbf{X}$ for any $\lambda \neq 0$, then

\begin{equation}
    \underset{\begin{subarray}{c} \mathbf{A} \in F(N, K) \\ \mathbf{B} \in F(K, N) \end{subarray}}{\arg \min} \| \tilde{\mathbf{X}} - \mathbf{A} \mathbf{B} \tilde{\mathbf{X}} \|_F^2 
    = 
    \underset{\begin{subarray}{c} \mathbf{A} \in F(N, K) \\ \mathbf{B} \in F(K, N) \end{subarray}}{\arg \min} \| \mathbf{X} - \mathbf{A} \mathbf{B} \mathbf{X}\|_F^2 
\end{equation}

Proof: Let $\lambda \neq 0$, and let $\tilde{\mathbf{X}} = \lambda \mathbf{X}$ be the scaled matrix. Then for any feasible $\mathbf{A}, \mathbf{B}$

\begin{equation}
    \begin{aligned}
    \tilde{\mathbf{X}} - \mathbf{A} \mathbf{B} \tilde{\mathbf{X}} 
    &= \lambda \mathbf{X} - \mathbf{A} \mathbf{B} \lambda \mathbf{X} \\
    &= \lambda \left( \mathbf{X} - \mathbf{A} \mathbf{B} \mathbf{X} \right)
    \end{aligned}
\end{equation}

Thus the objective for the scaled matrix is given by

\begin{equation}
    \underset{\begin{subarray}{c} \mathbf{A} \in F(N, K) \\ \mathbf{B} \in F(K, N) \end{subarray}}{\arg \min} \| \tilde{\mathbf{X}} - \mathbf{A} \mathbf{B} \tilde{\mathbf{X}} \|_F^2 = \underset{\begin{subarray}{c} \mathbf{A} \in F(N, K) \\ \mathbf{B} \in F(K, N) \end{subarray}}{\arg \min} \lambda^2 \| \mathbf{X} - \mathbf{A} \mathbf{B} \mathbf{X} \|_F^2
\end{equation}

Since we assumed $\lambda \neq 0$, $\lambda^2$ will always be a positive scalar. Multiplying the objective by any positive scalar does not change the location of its minimum, since the ordering of function values is preserved. Thus we have

\begin{equation}
    \underset{\begin{subarray}{c} \mathbf{A} \in F(N, K) \\ \mathbf{B} \in F(K, N) \end{subarray}}{\arg \min} \| \tilde{\mathbf{X}} - \mathbf{A} \mathbf{B} \tilde{\mathbf{X}} \|_F^2 = \underset{\begin{subarray}{c} \mathbf{A} \in F(N, K) \\ \mathbf{B} \in F(K, N) \end{subarray}}{\arg \min} \| \mathbf{X} - \mathbf{A} \mathbf{B} \mathbf{X} \|_F^2
\end{equation}


Property 3 (Rewrite using convex hull of $\mathbf{Z}$)

Proof, 

\section{Optimization}

While this objective is an NP-hard Euclidean sum of square clustering problem \autocite{aloiseNPhardnessEuclideanSumofsquares2009}, several practical optimization approaches have been developed that exploit that this objective is biconvex, meaning that it is convex in $\mathbf{A}$ if we fix $\mathbf{B}$ and vice versa. See Section 5 in Cutler \& Breiman (1994) \autocite{cutlerArchetypalAnalysis1994} or Section 2 in Mørup \& Hansen (2012) \autocite{morupArchetypalAnalysisMachine2012}. One way to optimize such a biconvex objective is to initialize $\mathbf{A}$, $\mathbf{B}$, and then alternating between solving the convex optimization problem in one variable fixing the other variable, and vice versa.

\subsection{Gradient of the Objective}

To compute the gradient of the unconstrained objective w.r.t. $\mathbf{A}$ and $\mathbf{B}$, we first rewrite the residual sum of squares (Frobenius norm) in Equation~\eqref{eq:objective} in terms of the trace
\begin{equation}
\begin{aligned}
\operatorname{RSS} &= \| \mathbf{X} - \mathbf{A} \mathbf{B} \mathbf{X}\|_F^2 \\
&= \operatorname{tr} \left( \left( \mathbf{X} - \mathbf{A} \mathbf{B} \mathbf{X} \right)^T \left( \mathbf{X} - \mathbf{A} \mathbf{B} \mathbf{X} \right) \right) \\
&=  \operatorname{tr}(\mathbf{X}^T \mathbf{X}) - \operatorname{tr}(\mathbf{X}^T \mathbf{A} B \mathbf{X}) - \operatorname{tr}(\mathbf{X}^T \mathbf{B}^T \mathbf{A}^T \mathbf{X}) + \operatorname{tr}(\mathbf{X}^T \mathbf{B}^T \mathbf{A}^T \mathbf{A} \mathbf{B} \mathbf{X}) \\
&=  \operatorname{tr}(\mathbf{X}^T \mathbf{X}) - 2 \operatorname{tr}(\mathbf{X}^T \mathbf{A} \mathbf{B} \mathbf{X}) + \operatorname{tr}(\mathbf{X}^T \mathbf{B}^T \mathbf{A}^T \mathbf{A} \mathbf{B} \mathbf{X}) \\
\end{aligned}
\tag{5}
\end{equation}
where we used that for any $\mathbf{G}, \mathbf{H} \in \mathbb{R}^{N \times N}$ it is true that $\operatorname{tr}(\mathbf{G} + \mathbf{H}) = \operatorname{tr}(\mathbf{G})+\operatorname{tr}(\mathbf{H})$ and $\operatorname{tr}(\mathbf{G}^T)=\operatorname{tr}(\mathbf{G})$

Next we will use Equation 101 from the Matrix Cookbook by Petersen and Pedersen (2012) \autocite{petersenMatrixCookbook2012} which states that for any matrices $G, H, J \in \mathbb{R}^{N \times N}$ we have
\begin{equation}
\frac{\partial}{\partial H}\operatorname{tr}(GHJ) = G^T J^T
\end{equation}
and Equation 116 which states that for any matrices $G, H, J \in \mathbb{R}^{N \times N}$ we have
\begin{equation}
\frac{\partial}{\partial H}\operatorname{tr}(G^T H^T J H G) = J^T H G G^T + J H G G^T
\end{equation}
So computing the gradient of the RSS w.r.t. $A$ we have
\begin{equation}
\begin{aligned}
G^{(A)} &= \nabla_A \operatorname{RSS} \\
&=\nabla_A \left[ \operatorname{tr}(X^T X) - 2 \operatorname{tr}(X^T A B X) + \operatorname{tr}(X^T B^T A^T A B X) \right] \\
&= - 2 \nabla_A \operatorname{tr}(\underbrace{X^T}_{G} \underbrace{A}_{H} \underbrace{B X}_{J}) + \nabla_A \operatorname{tr}(\underbrace{(B X)^T}_{G^T} \underbrace{A^T}_{H^T} \underbrace{I}_{J} \underbrace{A}_{H} \underbrace{B X}_{G}) \\
&= - 2 X X^T B^T + \left( I^T A B X X^T B^T + I A B X X^T B^T \right) \\
&= - 2 X X^T B^T + 2 A B X X^T B^T \\
&= 2 \left( A B X X^T B^T - X X^T B^T \right) \\
&= 2 \left( A Z Z^T - X Z^T \right)
\end{aligned}
\end{equation}
Similarly, computing the gradient of the RSS w.r.t. $B$ we have
\begin{equation}
\begin{aligned}
G^{(B)} &= \nabla_B \operatorname{RSS} \\
&= \nabla_A \left[ \operatorname{tr}(X^T X) - 2 \operatorname{tr}(X^T A B X) + \operatorname{tr}(X^T B^T A^T A B X) \right] \\
&= - 2 \nabla_B \operatorname{tr}(\underbrace{X^T A}_{G} \underbrace{B}_{H} \underbrace{X}_{J}) + \nabla_B \operatorname{tr}(\underbrace{X^T}_{G^T} \underbrace{B^T}_{H^T} \underbrace{A^T A}_{J} \underbrace{B}_{H} \underbrace{X}_{G}) \\
& = - 2 A^T X X^T + \left( A^T A B X X^T + A^T A B X X^T \right) \\
& = - 2 A^T X X^T + 2 A^T A B X X^T  \\
& = 2 \left( A^T A B X X^T - A^T X X^T \right)
\end{aligned}
\end{equation}

\subsection{Regularized Nonnegative Least Squares}

Introduced in 1994 by Adele Cutler and Leo Breiman \autocite{cutlerArchetypalAnalysis1994}, this was the first algorithm to solve the archetypal analysis objective in Equation~\eqref{eq:objective}.

\begin{algorithm}
\caption{Archetypal Analysis Algorithm}
\begin{algorithmic}[1]
\State Initialize $\mathbf{B}$ and compute the archetypes $\mathbf{Z} = \mathbf{B} \mathbf{X}$
\While{not converged or maximum number of iterations is reached}
    \For{$n = 1$ to $N$}
        \State Find optimal $\mathbf{a}_n$ by solving the constrained optimization problem:
        $$\mathbf{a}_n = \underset{\mathbf{a}_n \in \mathbb{R}^K}{\arg \min} \|\mathbf{x}_n - \mathbf{Z}^T \mathbf{a}_n\|_2^2 \quad \text{ subject to } \quad \mathbf{a}_n \geq 0, \sum_{k=1}^K a_{nk} = 1$$
    \EndFor
    \State Compute the optimal archetypes $\mathbf{Z}$ given $\mathbf{A}$, i.e.
    $$\mathbf{Z} = \underset{\mathbf{Z} \in \mathbb{R}^{K \times D}}{\arg \min} \| \mathbf{X} - \mathbf{A} \mathbf{Z}\|_F^2$$
    \For{$k = 1$ to $K$}
        \State Find optimal $\mathbf{b}_k$ by solving the constrained optimization problem:
        $$\mathbf{b}_k = \underset{\mathbf{b}_k \in \mathbb{R}^N}{\arg \min} \|\mathbf{z}_k - \mathbf{X}^T \mathbf{b}_k\|_2^2 \quad \text{ subject to } \quad \mathbf{b}_k \geq 0, \sum_{n=1}^N b_{kn} = 1$$
    \EndFor
    \State Compute the archetypes given $\mathbf{B}$, i.e. $\mathbf{Z}=\mathbf{B}\mathbf{X}$
\EndWhile
\State \Return $\mathbf{A}, \mathbf{B}, \mathbf{Z}$
\end{algorithmic}
\end{algorithm}

The authors originally proposed to solve the constrained optimization problems using a Nonnegative Least Squares Problem (NNLS) solver and enforcing the convexity constraints using a penalty term with regularization parameter $\lambda$, i.e.
\begin{equation}
\begin{aligned}
\mathbf{a}_n &= \underset{a_n \in \mathbb{R}^K}{\arg \min} \|x_n - Z^T \mathbf{a}_n\|_2^2 + \lambda \| \mathbf{1}_K - \mathbf{a}_n \|_2^2 \quad \text{ subject to } \quad \mathbf{a}_n \geq 0 \\
&= \underset{\mathbf{a}_n \in \mathbb{R}^K}{\arg \min} \left\| \begin{bmatrix} \mathbf{x}_n \\ \lambda \end{bmatrix} -  \begin{bmatrix} \mathbf{Z}^T \\ \lambda \mathbf{1}_K^T \end{bmatrix} \mathbf{a}_n \right\|_2^2
\end{aligned}
\end{equation}

Equivalently, for $\mathbf{B}$ we have
\begin{equation}
\begin{aligned}
\mathbf{b}_k &= \underset{b_k \in \mathbb{R}^N}{\arg \min} \|\mathbf{z}_k - \mathbf{X}^T \mathbf{b}_k\|_2^2 + \lambda \| \mathbf{1}_N - \mathbf{b}_k \|_2^2 \quad \text{ subject to } \quad \mathbf{b}_k \geq 0 \\
&= \underset{\mathbf{a}_n \in \mathbb{R}^K}{\arg \min} \left\| \begin{bmatrix} \mathbf{z}_k \\ \lambda \end{bmatrix} -  \begin{bmatrix} X^T \\ \lambda \mathbf{1}_N^T \end{bmatrix} b_k \right\|_2^2
\end{aligned}
\end{equation}

\subsection{Principal Convex Hull Algorithm (PCHA)}

Inspired by the projected gradient method for NMF \autocite{linProjectedGradientMethods2007} and normalization invariance approach introduced for NMF \autocite{eggertSparseCodingNMF2004}, the PCHA algorithm was introduced by Morten Mørup and Lars Kai Hansen in 2012 to solve the archetypal analysis objective. 

The idea is to use a projected gradient algorithm to solve the objective in Equation~\eqref{eq:objective}.

First, we recast the optimization problem in terms of the l1-normalization invariant variables $\tilde{a}_n$ and $\tilde{b}_k$ (called invariant because these variables won't change if one applies l1-normalization)
\begin{equation}
\tilde{a}_{nk} = \frac{a_{nk}}{\sum_{k''=1}^K a_{nk''}}, \quad \tilde{b}_{kn} = \frac{b_{kn}}{\sum_{n''=1}^N b_{kn''}}
\end{equation}
Then the gradient of the RSS wrt to $a_{n}$ is obtained using the chain rule which yields
\begin{equation}
\begin{aligned}
\frac{\partial \text{RSS}}{\partial a_n} &= \frac{\partial \text{RSS}}{\partial \tilde{a}_n} \frac{\partial  \tilde{a}_n}{\partial a_n} \\
&= \left( \tilde{g}^{(A)}_n \right)^T \left( \frac{\left( \sum_{k''=1}^K a_{nk''} \right) \mathbf{I}_K - a_n \mathbf{1}_K^T}{\left( \sum_{k''=1}^K a_{nk''} \right)^2} \right) \\
&= \frac{\left( \sum_{k''=1}^K a_{nk''} \right) \left( \tilde{g}^{(A)}_n \right)^T \mathbf{I}_K - \left( \tilde{g}^{(A)}_n \right)^T a_n \mathbf{1}_K^T}{\left( \sum_{k''=1}^K a_{nk''} \right)^2}
\end{aligned}
\end{equation}
So for a single element we have
\begin{equation}
\begin{aligned}
\frac{\partial \text{RSS}}{\partial a_{nk}} &= \frac{\partial \text{RSS}}{\partial \tilde{a}_n} \frac{\partial  \tilde{a}_n}{\partial a_{nk}} \\
&= \frac{\left( \sum_{k''=1}^K a_{nk''} \right) \tilde{g}^{(A)}_{nk} - \left( \tilde{g}^{(A)}_n \right)^T a_n}{\left( \sum_{k''=1}^K a_{nk''} \right)^2} \\
&= \frac{\left( \sum_{k''=1}^K a_{nk''} \right) \tilde{g}^{(A)}_{nk} - \sum_{k''=1}^K  \tilde{g}^{(A)}_{n k^{\prime \prime}} a_{nk''} }{\left( \sum_{k''=1}^K a_{nk''} \right)^2} \\
\end{aligned}
\end{equation}
IF we additionally assume that $a_n$ has been l1 normalized in the previous iteration we get 
\begin{equation}
\begin{aligned}
\frac{\partial \text{RSS}}{\partial a_{nk}} 
&= \tilde{g}^{(A)}_{nk} - \sum_{k''=1}^K  \tilde{g}^{(A)}_{n k^{\prime \prime}} a_{nk''} \\
\end{aligned}
\end{equation}
which is exactly the same as in Section 2.2. of Mørup \& Hansen (2012) \autocite{morupArchetypalAnalysisMachine2012}

To write down the algorithm we define $P_{\Sigma_M}$, a function that projects the rows of any matrix $\mathbf{H} \in \mathbb{R}^{N \times M}$ onto the $M$ simplex
\begin{equation}
\begin{aligned}
\tilde{\mathbf{H}} &= P_{\Sigma_M} \left(\mathbf{H} \right) \quad \text{with}\\
\tilde{\mathbf{H}}_{nm} &= \frac{\max(\mathbf{H}_{nm},0)}{\sum_{m^\prime=1}^M \max(\mathbf{H}_{nm^\prime},0)}
\end{aligned}
\end{equation}
Putting everything together, the algorithm in matrix notation is shown in Algorithm~\ref{alg:pcha}

\begin{algorithm}
\caption{Principal Convex Hull Algorithm (PCHA)}
\label{alg:pcha}
\begin{algorithmic}[1]
\State Initialize $\tilde{\mathbf{A}}$, $\tilde{\mathbf{B}}$
\State Initialize $\mu_\mathbf{A} \gets 1$, $\mu_\mathbf{B} \gets 1$
  
\While{not converged or maximum number of iterations is reached}
    \State \fbox{\parbox{0.95\linewidth}{Update $\mathbf{A}$ using projected gradient descent}}
    \State $\mathbf{Z} \gets \tilde{\mathbf{B}} \mathbf{X}$
    \State $\text{RSS}_{\text{old}} \gets \| \mathbf{X} - \mathbf{A} \mathbf{Z} \|_F^2$

    \For{$t = 1$ to $T$}
        \State $\tilde{\mathbf{G}}^{(\mathbf{A})} \gets 2 \left( \tilde{\mathbf{A}} \mathbf{Z} \mathbf{Z}^T - \mathbf{X} \mathbf{Z}^T \right)$
        \State $\mathbf{G}^{(\mathbf{A})} \gets \tilde{\mathbf{G}}^{(\mathbf{A})} - \left(\tilde{\mathbf{G}}^{(\mathbf{A})} \odot \mathbf{A}\right) \mathbf{1}_K \mathbf{1}_K^T$
        
        \For{$j = 1$ to $100T$} \Comment{line search}
            \State $\mathbf{A} \gets \mathbf{A} - \mu_\mathbf{A} \mathbf{G}^{(\mathbf{A})}$
            \State $\tilde{\mathbf{A}} \gets P_{\Sigma_K}(\mathbf{A})$
            \State $\text{RSS}_{\text{new}} \gets \| \mathbf{X} - \tilde{\mathbf{A}} \mathbf{Z} \|_F^2$
            \If{$\text{RSS}_{\text{new}} < \text{RSS}_{\text{old}} + (1+\epsilon)$}
                \State $\mu_\mathbf{A} \gets 1.2 \cdot \mu_\mathbf{A}$
                \State \textbf{break}
            \Else
                \State $\mu_\mathbf{A} \gets 0.5 \cdot \mu_\mathbf{A}$
            \EndIf
        \EndFor
    \EndFor

    \State \fbox{\parbox{0.95\linewidth}{Update $\mathbf{B}$ using projected gradient descent}}
    \State $\text{RSS}_{\text{old}} \gets \| \mathbf{X} - \mathbf{A} \mathbf{B} \mathbf{X} \|_F^2$

    \For{$t = 1$ to $T$}
        \State $\tilde{\mathbf{G}}^{(\mathbf{B})} \gets 2 \left( \tilde{\mathbf{A}}^T \tilde{\mathbf{A}} \tilde{\mathbf{B}} \mathbf{X} \mathbf{X}^T - \tilde{\mathbf{A}}^T \mathbf{X} \mathbf{X}^T \right)$
        \State $\mathbf{G}^{(\mathbf{B})} \gets \tilde{\mathbf{G}}^{(\mathbf{B})} - \left( \tilde{\mathbf{G}}^{(\mathbf{B})} \odot \mathbf{B} \right) \mathbf{1}_N \mathbf{1}_N^T$
        
        \For{$j = 1$ to $100T$} \Comment{line search}
            \State $\mathbf{B} \gets \mathbf{B} - \mu_\mathbf{B} \mathbf{G}^{(\mathbf{B})}$
            \State $\tilde{\mathbf{B}} \gets P_{\Sigma_N}(\mathbf{B})$
            \State $\text{RSS}_{\text{new}} \gets \| \mathbf{X} - \tilde{\mathbf{A}} \tilde{\mathbf{B}} \mathbf{X} \|_F^2$
            \If{$\text{RSS}_{\text{new}} < \text{RSS}_{\text{old}} + (1+\epsilon)$}
                \State $\mu_\mathbf{B} \gets 1.2 \cdot \mu_\mathbf{B}$
                \State \textbf{break}
            \Else
                \State $\mu_\mathbf{B} \gets 0.5 \cdot \mu_\mathbf{B}$
            \EndIf
        \EndFor
    \EndFor

    \State \fbox{\parbox{0.95\linewidth}{Check for Convergence}}
    \State $\mathbf{Z} \gets \tilde{\mathbf{B}} \mathbf{X}$
    \State $\text{RSS} \gets \| \mathbf{X} - \tilde{\mathbf{A}} \mathbf{Z} \|_F^2$
    \If{RSS reduction is sufficient}
        \State \textbf{break}
    \EndIf
\EndWhile
\State \Return $\tilde{\mathbf{A}}, \tilde{\mathbf{B}}, \mathbf{Z}$
\end{algorithmic}
\end{algorithm}

\subsection{Frank-Wolfe Algorithm}

The idea of the Frank-Wolfe algorithm for archetypal analysis is to use gradient information, but to avoid the costly projection step of the PCHA. 

As described above, the objective is convex in $\mathbf{A}$ when fixing $\mathbf{B}$ and vice versa. Furthermore, in this alternating optimization setting, the rows of $\mathbf{A}$ and $\mathbf{B}$ are constrained to the $\Sigma_K$ and $\Sigma_N$ simplex, respectively, which are convex sets. Thus, we have a convex minimization problem over a convex set which can be tackled using the efficient Frank-Wolfe algorithm \autocite{clarksonCoresetsSparseGreedy2010}

\section{Initialization}

\subsection{Furthest Sum}

\section{References}

\printbibliography[heading=none]

\section{Appendix}

\subsection{Notation}

\begin{itemize}
    \item $N \in \mathbb{N}$ is the number of samples
    \item $D \in \mathbb{N}$ is the number of dimensions
    \item $K \leq \min(N, D)$ is the number of archetypes
    \item $\mathcal{X}=\{\mathbf{x}_1, \ldots, \mathbf{x}_N\}_{n=1}^N$ is our dataset, where each $\mathbf{x}_n \in \mathbb{R}^D$
    \item $\mathbf{X} \in \mathbb{R}^{N \times D}$ is our data matrix where each row is one sample
    \item $\mathbf{Z} \in \mathbb{R}^{K \times D}$ is our matrix of archetypes where each row is one archetype
\end{itemize}

\subsection{Algorithms}

\begin{algorithm}
\caption{Principal Convex Hull Algorithm (PCHA)}
\label{alg:pcha}
\begin{algorithmic}[1]
\Require Data matrix $\mathbf{X} \in \mathbb{R}^{N \times D}$, learning rates $\mu_{\mathbf{A}} > 0$, $\mu_{\mathbf{B}} > 0$
\State Initialize $\mathbf{A}, \mathbf{B}$
\State $\text{RSS}_{\text{old}} \gets \| \mathbf{X} - \mathbf{A} \mathbf{B} \mathbf{X} \|_F^2$
\While{not converged}
    \Statex \hspace{-\algorithmicindent} \textbf{Update A coefficients:}
    \State $\mathbf{Z} \gets \mathbf{B} \mathbf{X}$ \Comment{compute archetypes}
    \State $\mathbf{G}^{(\mathbf{A})} \gets 2(\mathbf{A} \mathbf{Z} \mathbf{Z}^T - \mathbf{X} \mathbf{Z}^T)$ \Comment{gradient of RSS w.r.t. $\mathbf{A}$}
    \State $\mathbf{A} \gets \mathbf{A} - \mu_{\mathbf{A}} \mathbf{G}^{(\mathbf{A})}$ \Comment{gradient descent step}
    \State $\mathbf{A} \gets P_{\Sigma_K}(\mathbf{A})$ \Comment{project rows of $\mathbf{A}$ onto $K$-simplex}
    \Statex \hspace{-\algorithmicindent} \textbf{Update B coefficients:}
    \State $\mathbf{G}^{(\mathbf{B})} \gets 2(\mathbf{A}^T \mathbf{A} \mathbf{B} \mathbf{X} \mathbf{X}^T - \mathbf{A}^T \mathbf{X} \mathbf{X}^T)$ \Comment{gradient of RSS w.r.t. $\mathbf{B}$}
    \State $\mathbf{B} \gets \mathbf{B} - \mu_{\mathbf{B}} \mathbf{G}^{(\mathbf{B})}$ \Comment{gradient descent step}
    \State $\mathbf{B} \gets P_{\Sigma_N}(\mathbf{B})$ \Comment{project rows of $\mathbf{B}$ onto $N$-simplex}
    \Statex \hspace{-\algorithmicindent} \textbf{Check convergence:}
    \State $\text{RSS}_{\text{new}} \gets \| \mathbf{X} - \mathbf{A} \mathbf{B} \mathbf{X} \|_F^2$
    \State $\text{rel\_decrease} \gets \frac{\text{RSS}_{\text{old}} - \text{RSS}_{\text{new}}}{\text{RSS}_{\text{old}}}$ \Comment{relative decrease in RSS}
    \If{$\text{rel\_decrease} < \epsilon$}
        \State \textbf{break} \Comment{convergence criterion met}
    \EndIf
    \State $\text{RSS}_{\text{old}} \gets \text{RSS}_{\text{new}}$
\EndWhile
\State \Return $\mathbf{A}, \mathbf{B}, \mathbf{Z}$
\end{algorithmic}
\end{algorithm}

\end{document}